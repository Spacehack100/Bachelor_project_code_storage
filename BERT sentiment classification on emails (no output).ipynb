{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f465bad9",
   "metadata": {},
   "source": [
    "# Import necassary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import ast\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "torch.cuda.is_available()\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff4bd303",
   "metadata": {},
   "source": [
    "# Retrieve Huggingface dataset and prepare it for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac399f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainSet = pd.read_parquet('dutch_social dataset\\dutch_social-train.parquet')\n",
    "valSet = pd.read_parquet('dutch_social dataset\\dutch_social-validation.parquet')\n",
    "testSet = pd.read_parquet('dutch_social dataset\\dutch_social-test.parquet')\n",
    "trainSet = trainSet[['full_text','label']]\n",
    "valSet = valSet[['full_text','label']]\n",
    "testSet = testSet[['full_text','label']]\n",
    "trainSet.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "558734b2",
   "metadata": {},
   "source": [
    "# Retrieve own dataset and prepare it for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('test_sentiment_emails_labeled_V4.csv')\n",
    "dataset = dataset[['text','label']]\n",
    "dataset = dataset.dropna()\n",
    "dataset['label'] = dataset['label'].astype(int)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6a73dfc",
   "metadata": {},
   "source": [
    "### Split dataset into train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eeaf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, evalSet = train_test_split(dataset, test_size=0.25, stratify=dataset['label'], random_state=42)\n",
    "valSet, testSet = train_test_split(evalSet, test_size=0.4, stratify=evalSet['label'], random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83b1e343",
   "metadata": {},
   "source": [
    "### Define function for calculating class weights + check for usable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if CUDA capable gpu is available\n",
    "cudaAvailable = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def calculate_class_weights(trainingDataset):\n",
    "    weightList = []\n",
    "\n",
    "    # Get number of labels\n",
    "    numberOfLabels = len(trainingDataset['label'].unique())\n",
    "    for i in range(0,numberOfLabels):\n",
    "\n",
    "        # Calculate class weight with total label count and label count of that class\n",
    "        weight = len(trainingDataset.index) / (numberOfLabels * len(trainingDataset[trainingDataset['label'] == i].index))\n",
    "        weightList.append(weight)\n",
    "    print(weightList)\n",
    "    return weightList"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34b89668",
   "metadata": {},
   "source": [
    "# Define model/sweep and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5662e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configuration of sweep methods and parameters\n",
    "sweepConfig = {'method' : 'grid', 'parameters' : {'num_train_epochs' : {'min' : 3, 'max' : 5}, 'learning_rate' : {'values' : [4e-5,5e-5,6e-5]}, 'train_batch_size' : {'values' : [16,32]}}}\n",
    "sweepID = wandb.sweep(sweepConfig, project='Sentiment_email_sweep_test')\n",
    "\n",
    "# Define function for calculating F1-score using method from Scikit-learn\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "\n",
    "# Function to start sweep\n",
    "def train():\n",
    "    wandb.init()\n",
    "\n",
    "    #Further configure sweep\n",
    "    trainArgumentsSweep = {'reprocess_input_data' : True, 'use_multiprocessing' : True, 'overwrite_output_dir' : True, 'use_early_stopping' : True, 'early_stopping_consider_epochs' : True, 'optimizer' : 'AdamW', 'save_model_every_epoch' : False, 'wandb_project' : 'Test sweep'}\n",
    "    classifierSweep = ClassificationModel(\"bert\",\"GroNLP/bert-base-dutch-cased\", num_labels=len(trainSet['label'].unique()),args=trainArgumentsSweep, use_cuda=True, weight=calculate_class_weights(trainSet), sweep_config=wandb.config)\n",
    "    classifierSweep.train_model(trainSet)\n",
    "\n",
    "    # Tabulate results per sweep and log them\n",
    "    result, model_outputs, wrong_predictions = classifierSweep.eval_model(valSet, f1=f1_multiclass, confusionMatrix=confusion_matrix)\n",
    "    wandb.log({'mcc' : result['mcc'], 'f1' : result['f1'], 'confusion_matrix' : result['confusionMatrix']})\n",
    "    wandb.join()\n",
    "\n",
    "# Start sweep     \n",
    "wandb.agent(sweepID, train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81af7691",
   "metadata": {},
   "source": [
    "### Move best result into sepperate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a37df0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project='Sentiment_email_test')\n",
    "trainArguments = {'reprocess_input_data' : True, 'use_multiprocessing' : True, 'num_train_epochs' : 5, 'overwrite_output_dir' : True, 'use_early_stopping' : True, 'early_stopping_consider_epochs' : True, 'train_batch_size' : 64, 'optimizer' : 'AdamW', 'save_model_every_epoch' : False, 'logging_steps' : 25, 'learning_rate' : 5e-05, 'wandb_kwargs' : {'magic' : True}}\n",
    "classifier = ClassificationModel(\"bert\",\"GroNLP/bert-base-dutch-cased\", num_labels=len(trainSet['label'].unique()),args=trainArguments, use_cuda=True, weight=calculate_class_weights(trainSet))\n",
    "classifier.train_model(trainSet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a7a8fc4",
   "metadata": {},
   "source": [
    "### Evaluate the resulting model against validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94ad9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "\n",
    "result, model_outputs, wrong_predictions = classifier.eval_model(valSet, f1=f1_multiclass, confusionMatrix=confusion_matrix)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a22c4c19",
   "metadata": {},
   "source": [
    "### Print out emails that were classified wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a6162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, x in enumerate(wrong_predictions):\n",
    "    wrong_predictions[i] = str(wrong_predictions[i])\n",
    "    #wrong_predictions[i] = re.sub(\"'\",\"\\\"\",wrong_predictions[i])\n",
    "for i, x in enumerate(wrong_predictions):\n",
    "    wrong_predictions[i] = ast.literal_eval(wrong_predictions[i])\n",
    "wrong_predictions_df = pd.DataFrame(wrong_predictions)\n",
    "wrong_predictions_df = wrong_predictions_df.rename(columns={'guid' : 'testSet_id', 'text_a' : 'text', 'text_b' : 'pred_label', 'label' : 'true_label'})\n",
    "for i, row in wrong_predictions_df.iterrows():   \n",
    "    wrong_predictions_df.iloc[i,2] = np.argmax(model_outputs[int(row['testSet_id'])])\n",
    "wrong_predictions_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee596767",
   "metadata": {},
   "source": [
    "### Evaluate the resulting model against test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d7027",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "\n",
    "result, model_outputs, wrong_predictions = classifier.eval_model(testSet, f1=f1_multiclass, confusionMatrix=confusion_matrix)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee1fd3a6",
   "metadata": {},
   "source": [
    "### Print out emails that were classified wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a96b795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, x in enumerate(wrong_predictions):\n",
    "    wrong_predictions[i] = str(wrong_predictions[i])\n",
    "    #wrong_predictions[i] = re.sub(\"'\",\"\\\"\",wrong_predictions[i])\n",
    "for i, x in enumerate(wrong_predictions):\n",
    "    wrong_predictions[i] = ast.literal_eval(wrong_predictions[i])\n",
    "wrong_predictions_df = pd.DataFrame(wrong_predictions)\n",
    "wrong_predictions_df = wrong_predictions_df.rename(columns={'guid' : 'testSet_id', 'text_a' : 'text', 'text_b' : 'pred_label', 'label' : 'true_label'})\n",
    "for i, row in wrong_predictions_df.iterrows():   \n",
    "    wrong_predictions_df.iloc[i,2] = np.argmax(model_outputs[int(row['testSet_id'])])\n",
    "wrong_predictions_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "daeef7e1",
   "metadata": {},
   "source": [
    "### Save model localy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa07a1b5",
   "metadata": {},
   "source": [
    "classifier.save_model(os.getcwd() + '\\Bert model storage')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8388c24a",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "BERTje is capable of properly classifying mails based on their sentiment. The difference in performance between both datasets is caused by their size, which is simmilar beheviour with subject classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d33bb9c4",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "- https://huggingface.co/GroNLP/bert-base-dutch-cased\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html\n",
    "- https://huggingface.co/docs/transformers/perf_train_gpu_one\n",
    "- https://www.analyticsvidhya.com/blog/2020/10/improve-class-imbalance-class-weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1128bbc",
   "metadata": {},
   "source": [
    "# License"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10591551",
   "metadata": {},
   "source": [
    "BERTje: de Vries, W., van Cranenburgh, A., Bisazza, A., Caselli, T., van Noord, G., & Nissim, M. (2019, December 19). BERTje: A Dutch BERT Model. Groningen, Groningen, Nederland."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
